
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>pca_gen</title><meta name="generator" content="MATLAB 9.1"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-04-21"><meta name="DC.source" content="pca_gen.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">Step 0a: Load data</a></li><li><a href="#3">Step 0b: Zero-mean the data (by row)</a></li><li><a href="#4">Step 1a: Implement PCA to obtain xRot</a></li><li><a href="#5">Step 1b: Check your implementation of PCA</a></li><li><a href="#6">Step 2: Find k, the number of components to retain</a></li><li><a href="#7">Step 3: Implement PCA with dimension reduction</a></li><li><a href="#8">Step 4a: Implement PCA with whitening and regularisation</a></li><li><a href="#9">Step 4b: Check your implementation of PCA whitening</a></li><li><a href="#10">Step 5: Implement ZCA whitening</a></li></ul></div><pre class="codeinput"><span class="comment">%%================================================================</span>
</pre><h2 id="2">Step 0a: Load data</h2><pre>Here we provide the code to load natural image data into x.
x will be a 144 * 10000 matrix, where the kth column x(:, k) corresponds to
the raw image data from the kth 12x12 image patch sampled.
You do not need to change the code below.</pre><pre class="codeinput">close <span class="string">all</span>
x = sampleIMAGESRAW();
figure(<span class="string">'name'</span>,<span class="string">'Raw images'</span>);
randsel = randi(size(x,2),200,1); <span class="comment">% A random selection of samples for visualization</span>
display_network(x(:,randsel));

<span class="comment">%%================================================================</span>
</pre><img vspace="5" hspace="5" src="pca_gen_01.png" alt=""> <h2 id="3">Step 0b: Zero-mean the data (by row)</h2><pre>You can make use of the mean and repmat/bsxfun functions.</pre><pre class="codeinput"><span class="comment">% -------------------- YOUR CODE HERE --------------------</span>
x = x - repmat(mean(x,1), size(x,1),1);

<span class="comment">%%================================================================</span>
</pre><h2 id="4">Step 1a: Implement PCA to obtain xRot</h2><pre>Implement PCA to obtain xRot, the matrix in which the data is expressed
with respect to the eigenbasis of sigma, which is the matrix U.</pre><pre class="codeinput"><span class="comment">% -------------------- YOUR CODE HERE --------------------</span>
xRot = zeros(size(x)); <span class="comment">% You need to compute this</span>
sigma = x * x' / size(x, 2);
[U,S,V] = svd(sigma);

xRot(:,:) = U' * x;     <span class="comment">% rotated version of the data.</span>

<span class="comment">%%================================================================</span>
</pre><h2 id="5">Step 1b: Check your implementation of PCA</h2><pre>The covariance matrix for the data expressed with respect to the basis U
should be a diagonal matrix with non-zero entries only along the main
diagonal. We will verify this here.
Write code to compute the covariance matrix, covar.
When visualised as an image, you should see a straight line across the
diagonal (non-zero entries) against a blue background (zero entries).</pre><pre class="codeinput"><span class="comment">% -------------------- YOUR CODE HERE --------------------</span>
covar = zeros(size(x, 1)); <span class="comment">% You need to compute this</span>
covar(:,:) = cov(xRot');

<span class="comment">% Visualise the covariance matrix. You should see a line across the</span>
<span class="comment">% diagonal against a blue background.</span>
figure(<span class="string">'name'</span>,<span class="string">'Visualisation of covariance matrix'</span>);
imagesc(covar);

<span class="comment">%%================================================================</span>
</pre><img vspace="5" hspace="5" src="pca_gen_02.png" alt=""> <h2 id="6">Step 2: Find k, the number of components to retain</h2><pre>Write code to determine k, the number of components to retain in order
to retain at least 99% of the variance.</pre><pre class="codeinput"><span class="comment">% -------------------- YOUR CODE HERE --------------------</span>
k = 144; <span class="comment">% Set k accordingly</span>
sqrt_k = sqrt(k);
variance_original = var(x,1);
variance_k = variance_original;
perc = min(variance_original ./ variance_k);
<span class="keyword">while</span> sqrt_k &gt; 0
    sqrt_k = sqrt_k - 1;
    xTilde = U(:,1:sqrt_k)' * x;
    variance_k = var(xTilde,1);
    perc = min(variance_original ./ variance_k);
    fprintf(<span class="string">'Variance ratio for %d is %f\n'</span>, sqrt_k ^ 2, perc);
    <span class="keyword">if</span> perc &lt; .99
        k = (sqrt_k + 1) ^ 2;
        <span class="keyword">break</span>;
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">%%================================================================</span>
</pre><pre class="codeoutput">Variance ratio for 121 is 0.078062
</pre><h2 id="7">Step 3: Implement PCA with dimension reduction</h2><pre>Now that you have found k, you can reduce the dimension of the data by
discarding the remaining dimensions. In this way, you can represent the
data in k dimensions instead of the original 144, which will save you
computational time when running learning algorithms on the reduced
representation.</pre><pre>Following the dimension reduction, invert the PCA transformation to produce
the matrix xHat, the dimension-reduced data with respect to the original basis.
Visualise the data and compare it to the raw data. You will observe that
there is little loss due to throwing away the principal components that
correspond to dimensions with low variation.</pre><pre class="codeinput"><span class="comment">% -------------------- YOUR CODE HERE --------------------</span>
xHat = U(:,1:k)' * x; <span class="comment">% reduced dimension representation of the data,</span>
                        <span class="comment">% where k is the number of eigenvectors to keep</span>

<span class="comment">% Visualise the data, and compare it to the raw data</span>
<span class="comment">% You should observe that the raw and processed data are of comparable quality.</span>
<span class="comment">% For comparison, you may wish to generate a PCA reduced image which</span>
<span class="comment">% retains only 90% of the variance.</span>

figure(<span class="string">'name'</span>,[<span class="string">'PCA processed images '</span>,sprintf(<span class="string">'(%d / %d dimensions)'</span>, k, size(x, 1)),<span class="string">''</span>]);
display_network(xHat(:,randsel));
figure(<span class="string">'name'</span>,<span class="string">'Raw images'</span>);
display_network(x(:,randsel));

<span class="comment">%%================================================================</span>
</pre><img vspace="5" hspace="5" src="pca_gen_03.png" alt=""> <img vspace="5" hspace="5" src="pca_gen_04.png" alt=""> <h2 id="8">Step 4a: Implement PCA with whitening and regularisation</h2><pre>Implement PCA with whitening and regularisation to produce the matrix
xPCAWhite.</pre><pre class="codeinput">epsilon = 0.1;
xPCAWhite = zeros(size(x));

<span class="comment">% -------------------- YOUR CODE HERE --------------------</span>
xPCAWhite(:,:) = diag(1./sqrt(diag(S) + epsilon)) * U' * x;

<span class="comment">%%================================================================</span>
</pre><h2 id="9">Step 4b: Check your implementation of PCA whitening</h2><pre>Check your implementation of PCA whitening with and without regularisation.
PCA whitening without regularisation results a covariance matrix
that is equal to the identity matrix. PCA whitening with regularisation
results in a covariance matrix with diagonal entries starting close to
1 and gradually becoming smaller. We will verify these properties here.
Write code to compute the covariance matrix, covar.</pre><pre>Without regularisation (set epsilon to 0 or close to 0),
when visualised as an image, you should see a red line across the
diagonal (one entries) against a blue background (zero entries).
With regularisation, you should see a red line that slowly turns
blue across the diagonal, corresponding to the one entries slowly
becoming smaller.</pre><pre class="codeinput"><span class="comment">% -------------------- YOUR CODE HERE --------------------</span>
covar(:,:) = cov(xPCAWhite');

<span class="comment">% Visualise the covariance matrix. You should see a red line across the</span>
<span class="comment">% diagonal against a blue background.</span>
figure(<span class="string">'name'</span>,<span class="string">'Visualisation of covariance matrix'</span>);
imagesc(covar);

<span class="comment">%%================================================================</span>
</pre><img vspace="5" hspace="5" src="pca_gen_05.png" alt=""> <h2 id="10">Step 5: Implement ZCA whitening</h2><pre>Now implement ZCA whitening to produce the matrix xZCAWhite.
Visualise the data and compare it to the raw data. You should observe
that whitening results in, among other things, enhanced edges.</pre><pre class="codeinput">epsilon = 0.1;
xZCAWhite = zeros(size(x));
xZCAWhite = U * diag(1./sqrt(diag(S) + epsilon)) * U' * x;

<span class="comment">% -------------------- YOUR CODE HERE --------------------</span>

<span class="comment">% Visualise the data, and compare it to the raw data.</span>
<span class="comment">% You should observe that the whitened images have enhanced edges.</span>
figure(<span class="string">'name'</span>,<span class="string">'ZCA whitened images'</span>);
display_network(xZCAWhite(:,randsel));
figure(<span class="string">'name'</span>,<span class="string">'Raw images'</span>);
display_network(x(:,randsel));
</pre><img vspace="5" hspace="5" src="pca_gen_06.png" alt=""> <img vspace="5" hspace="5" src="pca_gen_07.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2016b</a><br></p></div><!--
##### SOURCE BEGIN #####
%%================================================================
%% Step 0a: Load data
%  Here we provide the code to load natural image data into x.
%  x will be a 144 * 10000 matrix, where the kth column x(:, k) corresponds to
%  the raw image data from the kth 12x12 image patch sampled.
%  You do not need to change the code below.
close all
x = sampleIMAGESRAW();
figure('name','Raw images');
randsel = randi(size(x,2),200,1); % A random selection of samples for visualization
display_network(x(:,randsel));

%%================================================================
%% Step 0b: Zero-mean the data (by row)
%  You can make use of the mean and repmat/bsxfun functions.

% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH 
x = x - repmat(mean(x,1), size(x,1),1);

%%================================================================
%% Step 1a: Implement PCA to obtain xRot
%  Implement PCA to obtain xRot, the matrix in which the data is expressed
%  with respect to the eigenbasis of sigma, which is the matrix U.


% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH 
xRot = zeros(size(x)); % You need to compute this
sigma = x * x' / size(x, 2);
[U,S,V] = svd(sigma);

xRot(:,:) = U' * x;     % rotated version of the data. 

%%================================================================
%% Step 1b: Check your implementation of PCA
%  The covariance matrix for the data expressed with respect to the basis U
%  should be a diagonal matrix with non-zero entries only along the main
%  diagonal. We will verify this here.
%  Write code to compute the covariance matrix, covar. 
%  When visualised as an image, you should see a straight line across the
%  diagonal (non-zero entries) against a blue background (zero entries).

% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH 
covar = zeros(size(x, 1)); % You need to compute this
covar(:,:) = cov(xRot');

% Visualise the covariance matrix. You should see a line across the
% diagonal against a blue background.
figure('name','Visualisation of covariance matrix');
imagesc(covar);

%%================================================================
%% Step 2: Find k, the number of components to retain
%  Write code to determine k, the number of components to retain in order
%  to retain at least 99% of the variance.

% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH 
k = 144; % Set k accordingly
sqrt_k = sqrt(k);
variance_original = var(x,1);
variance_k = variance_original;
perc = min(variance_original ./ variance_k);
while sqrt_k > 0
    sqrt_k = sqrt_k - 1;
    xTilde = U(:,1:sqrt_k)' * x;
    variance_k = var(xTilde,1);
    perc = min(variance_original ./ variance_k);
    fprintf('Variance ratio for %d is %f\n', sqrt_k ^ 2, perc);
    if perc < .99
        k = (sqrt_k + 1) ^ 2;
        break;
    end
end

%%================================================================
%% Step 3: Implement PCA with dimension reduction
%  Now that you have found k, you can reduce the dimension of the data by
%  discarding the remaining dimensions. In this way, you can represent the
%  data in k dimensions instead of the original 144, which will save you
%  computational time when running learning algorithms on the reduced
%  representation.
% 
%  Following the dimension reduction, invert the PCA transformation to produce 
%  the matrix xHat, the dimension-reduced data with respect to the original basis.
%  Visualise the data and compare it to the raw data. You will observe that
%  there is little loss due to throwing away the principal components that
%  correspond to dimensions with low variation.

% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH 
xHat = U(:,1:k)' * x; % reduced dimension representation of the data, 
                        % where k is the number of eigenvectors to keep

% Visualise the data, and compare it to the raw data
% You should observe that the raw and processed data are of comparable quality.
% For comparison, you may wish to generate a PCA reduced image which
% retains only 90% of the variance.

figure('name',['PCA processed images ',sprintf('(%d / %d dimensions)', k, size(x, 1)),'']);
display_network(xHat(:,randsel));
figure('name','Raw images');
display_network(x(:,randsel));

%%================================================================
%% Step 4a: Implement PCA with whitening and regularisation
%  Implement PCA with whitening and regularisation to produce the matrix
%  xPCAWhite. 

epsilon = 0.1;
xPCAWhite = zeros(size(x));

% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH 
xPCAWhite(:,:) = diag(1./sqrt(diag(S) + epsilon)) * U' * x;

%%================================================================
%% Step 4b: Check your implementation of PCA whitening 
%  Check your implementation of PCA whitening with and without regularisation. 
%  PCA whitening without regularisation results a covariance matrix 
%  that is equal to the identity matrix. PCA whitening with regularisation
%  results in a covariance matrix with diagonal entries starting close to 
%  1 and gradually becoming smaller. We will verify these properties here.
%  Write code to compute the covariance matrix, covar. 
%
%  Without regularisation (set epsilon to 0 or close to 0), 
%  when visualised as an image, you should see a red line across the
%  diagonal (one entries) against a blue background (zero entries).
%  With regularisation, you should see a red line that slowly turns
%  blue across the diagonal, corresponding to the one entries slowly
%  becoming smaller.

% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH 
covar(:,:) = cov(xPCAWhite');

% Visualise the covariance matrix. You should see a red line across the
% diagonal against a blue background.
figure('name','Visualisation of covariance matrix');
imagesc(covar);

%%================================================================
%% Step 5: Implement ZCA whitening
%  Now implement ZCA whitening to produce the matrix xZCAWhite. 
%  Visualise the data and compare it to the raw data. You should observe
%  that whitening results in, among other things, enhanced edges.
epsilon = 0.1;
xZCAWhite = zeros(size(x));
xZCAWhite = U * diag(1./sqrt(diag(S) + epsilon)) * U' * x;

% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH 

% Visualise the data, and compare it to the raw data.
% You should observe that the whitened images have enhanced edges.
figure('name','ZCA whitened images');
display_network(xZCAWhite(:,randsel));
figure('name','Raw images');
display_network(x(:,randsel));

##### SOURCE END #####
--></body></html>