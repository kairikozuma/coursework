
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>stlExercise</title><meta name="generator" content="MATLAB 9.1"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2017-04-11"><meta name="DC.source" content="stlExercise.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">CS294A/CS294W Self-taught Learning Exercise</a></li><li><a href="#2">======================================================================</a></li><li><a href="#3">======================================================================</a></li><li><a href="#4">======================================================================</a></li><li><a href="#5">----------------- YOUR CODE HERE ----------------------</a></li><li><a href="#6">-----------------------------------------------------</a></li><li><a href="#7">STEP 3: Extract Features from the Supervised Dataset</a></li><li><a href="#8">STEP 4: Train the softmax classifier</a></li><li><a href="#9">----------------- YOUR CODE HERE ----------------------</a></li><li><a href="#10">-----------------------------------------------------</a></li><li><a href="#11">STEP 5: Testing</a></li><li><a href="#12">----------------- YOUR CODE HERE ----------------------</a></li><li><a href="#13">-----------------------------------------------------</a></li></ul></div><h2 id="1">CS294A/CS294W Self-taught Learning Exercise</h2><pre class="codeinput"><span class="comment">%  Instructions</span>
<span class="comment">%  ------------</span>
<span class="comment">%</span>
<span class="comment">%  This file contains code that helps you get started on the</span>
<span class="comment">%  self-taught learning. You will need to complete code in feedForwardAutoencoder.m</span>
<span class="comment">%  You will also need to have implemented sparseAutoencoderCost.m and</span>
<span class="comment">%  softmaxCost.m from previous exercises.</span>
<span class="comment">%</span>
</pre><h2 id="2">======================================================================</h2><pre>STEP 0: Here we provide the relevant parameters values that will
allow your sparse autoencoder to get good filters; you do not need to
change the parameters below.</pre><pre class="codeinput">inputSize  = 28 * 28;
numLabels  = 5;
hiddenSize = 200;
sparsityParam = 0.1; <span class="comment">% desired average activation of the hidden units.</span>
                     <span class="comment">% (This was denoted by the Greek alphabet rho, which looks like a lower-case "p",</span>
		             <span class="comment">%  in the lecture notes).</span>
lambda = 3e-3;       <span class="comment">% weight decay parameter</span>
beta = 3;            <span class="comment">% weight of sparsity penalty term</span>
maxIter = 400;
</pre><h2 id="3">======================================================================</h2><pre>STEP 1: Load data from the MNIST database</pre><pre>This loads our training and test data from the MNIST database files.
We have sorted the data for you in this so that you will not have to
change it.</pre><pre class="codeinput"><span class="comment">% Load MNIST database files</span>
mnistData   = loadMNISTImages(<span class="string">'train-images-idx3-ubyte'</span>);
mnistLabels = loadMNISTLabels(<span class="string">'train-labels-idx1-ubyte'</span>);

<span class="comment">% Set Unlabeled Set (All Images)</span>

<span class="comment">% Simulate a Labeled and Unlabeled set</span>
labeledSet   = find(mnistLabels &gt;= 0 &amp; mnistLabels &lt;= 4);
unlabeledSet = find(mnistLabels &gt;= 5);

numTrain = round(numel(labeledSet)/2);
trainSet = labeledSet(1:numTrain);
testSet  = labeledSet(numTrain+1:end);

unlabeledData = mnistData(:, unlabeledSet);

trainData   = mnistData(:, trainSet);
trainLabels = mnistLabels(trainSet)' + 1; <span class="comment">% Shift Labels to the Range 1-5</span>

testData   = mnistData(:, testSet);
testLabels = mnistLabels(testSet)' + 1;   <span class="comment">% Shift Labels to the Range 1-5</span>

<span class="comment">% Output Some Statistics</span>
fprintf(<span class="string">'# examples in unlabeled set: %d\n'</span>, size(unlabeledData, 2));
fprintf(<span class="string">'# examples in supervised training set: %d\n\n'</span>, size(trainData, 2));
fprintf(<span class="string">'# examples in supervised testing set: %d\n\n'</span>, size(testData, 2));
</pre><pre class="codeoutput"># examples in unlabeled set: 29404
# examples in supervised training set: 15298

# examples in supervised testing set: 15298

</pre><h2 id="4">======================================================================</h2><pre>STEP 2: Train the sparse autoencoder
This trains the sparse autoencoder on the unlabeled training
images.</pre><pre class="codeinput"><span class="comment">%  Randomly initialize the parameters</span>
theta = initializeParameters(hiddenSize, inputSize);
</pre><h2 id="5">----------------- YOUR CODE HERE ----------------------</h2><pre>Find opttheta by running the sparse autoencoder on
unlabeledTrainingImages</pre><pre class="codeinput"><span class="comment">%[cost,grad] = sparseAutoencoderCost(theta, inputSize, hiddenSize, ...</span>
<span class="comment">%                                lambda, sparsityParam, beta, unlabeledData);</span>

<span class="comment">%  Use minFunc to minimize the function</span>
addpath <span class="string">minFunc/</span>
options.Method = <span class="string">'lbfgs'</span>; <span class="comment">% Here, we use L-BFGS to optimize our cost</span>
                          <span class="comment">% function. Generally, for minFunc to work, you</span>
                          <span class="comment">% need a function pointer with two outputs: the</span>
                          <span class="comment">% function value and the gradient. In our problem,</span>
                          <span class="comment">% sparseAutoencoderCost.m satisfies this.</span>
options.maxIter = maxIter;	  <span class="comment">% Maximum number of iterations of L-BFGS to run</span>
options.display = <span class="string">'on'</span>;


[opttheta, cost] = minFunc( @(p) sparseAutoencoderCost(p, <span class="keyword">...</span>
                                   inputSize, hiddenSize, <span class="keyword">...</span>
                                   lambda, sparsityParam, <span class="keyword">...</span>
                                   beta, unlabeledData), <span class="keyword">...</span>
                              theta, options);
</pre><pre class="codeoutput"> Iteration   FunEvals     Step Length    Function Val        Opt Cond
         1          5     3.17144e-02     1.09441e+02     8.39860e+03
         2          6     1.00000e+00     9.13105e+01     4.33276e+03
         3          7     1.00000e+00     8.38682e+01     1.95811e+03
         4          8     1.00000e+00     7.95454e+01     2.04678e+03
         5          9     1.00000e+00     7.11217e+01     2.73423e+03
         6         10     1.00000e+00     5.40402e+01     2.96698e+03
         7         11     1.00000e+00     3.26609e+01     7.77808e+02
         8         12     1.00000e+00     3.06043e+01     6.83039e+02
         9         13     1.00000e+00     2.97733e+01     6.03632e+02
        10         14     1.00000e+00     2.90295e+01     4.11155e+02
        11         15     1.00000e+00     2.85348e+01     2.28426e+02
        12         16     1.00000e+00     2.83625e+01     2.08767e+02
        13         17     1.00000e+00     2.83021e+01     1.55551e+02
        14         18     1.00000e+00     2.81605e+01     2.04181e+02
        15         19     1.00000e+00     2.80077e+01     2.96219e+02
        16         20     1.00000e+00     2.75822e+01     4.34911e+02
        17         21     1.00000e+00     2.71986e+01     4.55218e+02
        18         23     4.87538e-01     2.69981e+01     3.97166e+02
        19         24     1.00000e+00     2.67582e+01     1.93357e+02
        20         25     1.00000e+00     2.66034e+01     2.57440e+02
        21         26     1.00000e+00     2.64739e+01     3.13599e+02
        22         27     1.00000e+00     2.59104e+01     4.34788e+02
        23         28     1.00000e+00     2.54914e+01     5.91737e+02
        24         29     1.00000e+00     2.42665e+01     3.39863e+02
        25         30     1.00000e+00     2.35356e+01     2.27576e+02
        26         31     1.00000e+00     2.32985e+01     3.64472e+02
        27         32     1.00000e+00     2.30969e+01     2.50901e+02
        28         33     1.00000e+00     2.28323e+01     2.36672e+02
        29         34     1.00000e+00     2.23686e+01     3.43738e+02
        30         35     1.00000e+00     2.16783e+01     3.52177e+02
        31         37     3.22835e-01     2.13577e+01     4.50780e+02
        32         38     1.00000e+00     2.09310e+01     3.55776e+02
        33         39     1.00000e+00     2.06571e+01     2.64359e+02
        34         40     1.00000e+00     2.03842e+01     2.33227e+02
        35         41     1.00000e+00     2.01813e+01     2.52720e+02
        36         42     1.00000e+00     1.99626e+01     2.79159e+02
        37         43     1.00000e+00     1.96700e+01     2.90728e+02
        38         44     1.00000e+00     1.93207e+01     2.92083e+02
        39         45     1.00000e+00     1.88889e+01     2.75358e+02
        40         46     1.00000e+00     1.85137e+01     2.94104e+02
        41         47     1.00000e+00     1.81891e+01     1.99500e+02
        42         48     1.00000e+00     1.79962e+01     1.63231e+02
        43         49     1.00000e+00     1.78379e+01     1.66128e+02
        44         50     1.00000e+00     1.77576e+01     1.78648e+02
        45         51     1.00000e+00     1.76701e+01     1.56484e+02
        46         52     1.00000e+00     1.74868e+01     1.45945e+02
        47         53     1.00000e+00     1.72927e+01     1.46590e+02
        48         54     1.00000e+00     1.69987e+01     1.95222e+02
        49         55     1.00000e+00     1.67666e+01     1.54182e+02
        50         56     1.00000e+00     1.66386e+01     1.19332e+02
        51         57     1.00000e+00     1.65367e+01     1.23311e+02
        52         58     1.00000e+00     1.64760e+01     1.16718e+02
        53         59     1.00000e+00     1.63491e+01     1.15905e+02
        54         60     1.00000e+00     1.62727e+01     1.36567e+02
        55         61     1.00000e+00     1.61861e+01     1.28346e+02
        56         62     1.00000e+00     1.58852e+01     1.48303e+02
        57         63     1.00000e+00     1.58409e+01     2.11250e+02
        58         64     1.00000e+00     1.56707e+01     1.19305e+02
        59         65     1.00000e+00     1.55875e+01     8.81069e+01
        60         66     1.00000e+00     1.55047e+01     1.05132e+02
        61         67     1.00000e+00     1.54187e+01     1.33283e+02
        62         68     1.00000e+00     1.53333e+01     9.97891e+01
        63         69     1.00000e+00     1.52632e+01     8.57344e+01
        64         70     1.00000e+00     1.51760e+01     1.10322e+02
        65         71     1.00000e+00     1.50526e+01     1.30663e+02
        66         72     1.00000e+00     1.49164e+01     1.37132e+02
        67         73     1.00000e+00     1.48510e+01     1.21818e+02
        68         74     1.00000e+00     1.47939e+01     7.95476e+01
        69         75     1.00000e+00     1.47569e+01     8.16911e+01
        70         76     1.00000e+00     1.47125e+01     9.83483e+01
        71         77     1.00000e+00     1.46369e+01     1.11960e+02
        72         78     1.00000e+00     1.45889e+01     1.31148e+02
        73         79     1.00000e+00     1.44995e+01     8.05943e+01
        74         80     1.00000e+00     1.44228e+01     7.13846e+01
        75         81     1.00000e+00     1.43649e+01     8.76295e+01
        76         82     1.00000e+00     1.42973e+01     1.10089e+02
        77         83     1.00000e+00     1.42300e+01     9.35263e+01
        78         84     1.00000e+00     1.41729e+01     7.27321e+01
        79         85     1.00000e+00     1.41177e+01     7.28394e+01
        80         86     1.00000e+00     1.40919e+01     8.05346e+01
        81         87     1.00000e+00     1.40626e+01     6.70073e+01
        82         88     1.00000e+00     1.40121e+01     6.74240e+01
        83         89     1.00000e+00     1.39702e+01     7.83514e+01
        84         90     1.00000e+00     1.38887e+01     8.33154e+01
        85         91     1.00000e+00     1.38470e+01     1.31851e+02
        86         92     1.00000e+00     1.37793e+01     6.41942e+01
        87         93     1.00000e+00     1.37587e+01     4.97378e+01
        88         94     1.00000e+00     1.37413e+01     5.06827e+01
        89         95     1.00000e+00     1.37122e+01     6.29957e+01
        90         96     1.00000e+00     1.36643e+01     7.51976e+01
        91         97     1.00000e+00     1.36019e+01     7.70626e+01
        92         98     1.00000e+00     1.35514e+01     8.13900e+01
        93         99     1.00000e+00     1.35141e+01     5.67009e+01
        94        100     1.00000e+00     1.34972e+01     4.89847e+01
        95        101     1.00000e+00     1.34811e+01     4.81442e+01
        96        102     1.00000e+00     1.34643e+01     4.88345e+01
        97        103     1.00000e+00     1.34365e+01     5.82964e+01
        98        104     1.00000e+00     1.33949e+01     7.42400e+01
        99        105     1.00000e+00     1.33489e+01     7.88364e+01
       100        106     1.00000e+00     1.33131e+01     5.78690e+01
       101        107     1.00000e+00     1.32971e+01     4.35489e+01
       102        108     1.00000e+00     1.32899e+01     4.26242e+01
       103        109     1.00000e+00     1.32724e+01     5.79265e+01
       104        110     1.00000e+00     1.32382e+01     7.37887e+01
       105        111     1.00000e+00     1.31975e+01     8.47101e+01
       106        112     1.00000e+00     1.31617e+01     6.76440e+01
       107        113     1.00000e+00     1.31463e+01     3.71897e+01
       108        114     1.00000e+00     1.31441e+01     4.52361e+01
       109        115     1.00000e+00     1.31368e+01     4.06049e+01
       110        116     1.00000e+00     1.31232e+01     3.86121e+01
       111        117     1.00000e+00     1.30943e+01     4.36740e+01
       112        118     1.00000e+00     1.30547e+01     7.25499e+01
       113        119     1.00000e+00     1.30404e+01     7.24413e+01
       114        120     1.00000e+00     1.30216e+01     3.47914e+01
       115        121     1.00000e+00     1.30168e+01     3.01008e+01
       116        122     1.00000e+00     1.30053e+01     4.71663e+01
       117        123     1.00000e+00     1.29875e+01     6.02091e+01
       118        124     1.00000e+00     1.29576e+01     7.39105e+01
       119        125     1.00000e+00     1.29391e+01     7.86919e+01
       120        126     1.00000e+00     1.29173e+01     3.78457e+01
       121        127     1.00000e+00     1.29116e+01     3.14295e+01
       122        128     1.00000e+00     1.29055e+01     3.25049e+01
       123        129     1.00000e+00     1.28899e+01     4.67754e+01
       124        130     1.00000e+00     1.28648e+01     6.22186e+01
       125        131     1.00000e+00     1.28362e+01     7.54282e+01
       126        132     1.00000e+00     1.28143e+01     5.69712e+01
       127        133     1.00000e+00     1.28022e+01     3.48932e+01
       128        134     1.00000e+00     1.27951e+01     3.17301e+01
       129        135     1.00000e+00     1.27894e+01     3.96844e+01
       130        136     1.00000e+00     1.27692e+01     5.94217e+01
       131        137     1.00000e+00     1.27452e+01     7.15362e+01
       132        138     1.00000e+00     1.27237e+01     6.60430e+01
       133        139     1.00000e+00     1.27078e+01     3.66903e+01
       134        140     1.00000e+00     1.26982e+01     3.66899e+01
       135        141     1.00000e+00     1.26920e+01     3.98244e+01
       136        142     1.00000e+00     1.26764e+01     4.82656e+01
       137        143     1.00000e+00     1.26601e+01     4.67052e+01
       138        144     1.00000e+00     1.26448e+01     3.60643e+01
       139        145     1.00000e+00     1.26309e+01     4.03199e+01
       140        146     1.00000e+00     1.26206e+01     4.28611e+01
       141        147     1.00000e+00     1.26073e+01     3.87067e+01
       142        148     1.00000e+00     1.25953e+01     4.41942e+01
       143        149     1.00000e+00     1.25830e+01     3.22737e+01
       144        150     1.00000e+00     1.25735e+01     3.23726e+01
       145        151     1.00000e+00     1.25547e+01     3.29569e+01
       146        153     4.65618e-01     1.25482e+01     3.99788e+01
       147        154     1.00000e+00     1.25402e+01     3.07800e+01
       148        155     1.00000e+00     1.25257e+01     3.19838e+01
       149        156     1.00000e+00     1.25160e+01     3.70503e+01
       150        157     1.00000e+00     1.25022e+01     4.54284e+01
       151        158     1.00000e+00     1.24886e+01     3.79468e+01
       152        159     1.00000e+00     1.24772e+01     3.34333e+01
       153        160     1.00000e+00     1.24656e+01     3.63886e+01
       154        161     1.00000e+00     1.24582e+01     4.22217e+01
       155        162     1.00000e+00     1.24470e+01     3.70921e+01
       156        163     1.00000e+00     1.24311e+01     3.70221e+01
       157        164     1.00000e+00     1.24240e+01     4.08735e+01
       158        165     1.00000e+00     1.24173e+01     3.28163e+01
       159        166     1.00000e+00     1.24059e+01     3.23959e+01
       160        167     1.00000e+00     1.23968e+01     3.83387e+01
       161        168     1.00000e+00     1.23792e+01     4.56283e+01
       162        169     1.00000e+00     1.23690e+01     5.76724e+01
       163        170     1.00000e+00     1.23574e+01     2.99048e+01
       164        171     1.00000e+00     1.23522e+01     2.32979e+01
       165        172     1.00000e+00     1.23474e+01     2.78993e+01
       166        173     1.00000e+00     1.23368e+01     3.55870e+01
       167        174     1.00000e+00     1.23259e+01     4.90922e+01
       168        175     1.00000e+00     1.23121e+01     3.46394e+01
       169        176     1.00000e+00     1.23018e+01     2.95227e+01
       170        177     1.00000e+00     1.22950e+01     3.20335e+01
       171        178     1.00000e+00     1.22877e+01     4.16695e+01
       172        179     1.00000e+00     1.22792e+01     3.01005e+01
       173        180     1.00000e+00     1.22709e+01     2.65034e+01
       174        181     1.00000e+00     1.22653e+01     3.47687e+01
       175        182     1.00000e+00     1.22578e+01     3.84263e+01
       176        183     1.00000e+00     1.22449e+01     5.23783e+01
       177        184     1.00000e+00     1.22321e+01     3.57073e+01
       178        185     1.00000e+00     1.22250e+01     2.21090e+01
       179        186     1.00000e+00     1.22192e+01     2.72438e+01
       180        187     1.00000e+00     1.22141e+01     3.24657e+01
       181        188     1.00000e+00     1.22043e+01     3.52460e+01
       182        189     1.00000e+00     1.21979e+01     3.71010e+01
       183        190     1.00000e+00     1.21914e+01     2.56240e+01
       184        191     1.00000e+00     1.21835e+01     2.95552e+01
       185        192     1.00000e+00     1.21776e+01     3.51685e+01
       186        193     1.00000e+00     1.21681e+01     3.61205e+01
       187        194     1.00000e+00     1.21609e+01     3.08349e+01
       188        195     1.00000e+00     1.21560e+01     2.30935e+01
       189        196     1.00000e+00     1.21523e+01     2.66832e+01
       190        197     1.00000e+00     1.21447e+01     3.23463e+01
       191        198     1.00000e+00     1.21399e+01     4.22990e+01
       192        199     1.00000e+00     1.21327e+01     2.73734e+01
       193        200     1.00000e+00     1.21274e+01     2.24299e+01
       194        201     1.00000e+00     1.21228e+01     2.68224e+01
       195        202     1.00000e+00     1.21149e+01     3.49428e+01
       196        203     1.00000e+00     1.21097e+01     4.59669e+01
       197        204     1.00000e+00     1.21004e+01     2.75131e+01
       198        205     1.00000e+00     1.20952e+01     2.08180e+01
       199        206     1.00000e+00     1.20911e+01     2.34886e+01
       200        207     1.00000e+00     1.20858e+01     2.74159e+01
       201        208     1.00000e+00     1.20796e+01     3.77819e+01
       202        209     1.00000e+00     1.20715e+01     2.54450e+01
       203        210     1.00000e+00     1.20636e+01     2.23636e+01
       204        211     1.00000e+00     1.20575e+01     2.58904e+01
       205        212     1.00000e+00     1.20533e+01     3.81681e+01
       206        213     1.00000e+00     1.20473e+01     2.51278e+01
       207        214     1.00000e+00     1.20408e+01     2.26245e+01
       208        215     1.00000e+00     1.20354e+01     2.67980e+01
       209        216     1.00000e+00     1.20305e+01     3.10988e+01
       210        217     1.00000e+00     1.20247e+01     3.20597e+01
       211        218     1.00000e+00     1.20189e+01     2.61333e+01
       212        219     1.00000e+00     1.20122e+01     2.46619e+01
       213        220     1.00000e+00     1.20089e+01     2.69905e+01
       214        221     1.00000e+00     1.20051e+01     2.73438e+01
       215        222     1.00000e+00     1.19969e+01     2.94974e+01
       216        223     1.00000e+00     1.19931e+01     3.22959e+01
       217        224     1.00000e+00     1.19884e+01     1.94418e+01
       218        225     1.00000e+00     1.19851e+01     1.72650e+01
       219        226     1.00000e+00     1.19828e+01     2.32816e+01
       220        227     1.00000e+00     1.19782e+01     2.81806e+01
       221        228     1.00000e+00     1.19727e+01     3.13178e+01
       222        229     1.00000e+00     1.19665e+01     2.31490e+01
       223        230     1.00000e+00     1.19620e+01     1.86534e+01
       224        231     1.00000e+00     1.19584e+01     2.26570e+01
       225        232     1.00000e+00     1.19545e+01     2.41783e+01
       226        233     1.00000e+00     1.19486e+01     2.31072e+01
       227        234     1.00000e+00     1.19429e+01     1.97132e+01
       228        235     1.00000e+00     1.19394e+01     1.96226e+01
       229        236     1.00000e+00     1.19361e+01     1.82581e+01
       230        237     1.00000e+00     1.19318e+01     1.75178e+01
       231        238     1.00000e+00     1.19284e+01     2.39893e+01
       232        239     1.00000e+00     1.19240e+01     1.77991e+01
       233        240     1.00000e+00     1.19187e+01     1.65068e+01
       234        241     1.00000e+00     1.19160e+01     1.66442e+01
       235        242     1.00000e+00     1.19100e+01     1.93051e+01
       236        243     1.00000e+00     1.19042e+01     2.71138e+01
       237        244     1.00000e+00     1.18991e+01     2.34489e+01
       238        245     1.00000e+00     1.18956e+01     1.66204e+01
       239        246     1.00000e+00     1.18935e+01     1.60456e+01
       240        247     1.00000e+00     1.18907e+01     1.88599e+01
       241        248     1.00000e+00     1.18859e+01     2.51555e+01
       242        249     1.00000e+00     1.18807e+01     2.33193e+01
       243        250     1.00000e+00     1.18765e+01     1.67833e+01
       244        251     1.00000e+00     1.18737e+01     1.59808e+01
       245        252     1.00000e+00     1.18719e+01     1.76739e+01
       246        253     1.00000e+00     1.18680e+01     1.90760e+01
       247        254     1.00000e+00     1.18659e+01     3.93455e+01
       248        255     1.00000e+00     1.18587e+01     1.75974e+01
       249        256     1.00000e+00     1.18565e+01     1.32404e+01
       250        257     1.00000e+00     1.18544e+01     1.45881e+01
       251        258     1.00000e+00     1.18521e+01     2.19732e+01
       252        259     1.00000e+00     1.18483e+01     1.98702e+01
       253        260     1.00000e+00     1.18406e+01     2.06874e+01
       254        262     4.07391e-01     1.18384e+01     1.79742e+01
       255        263     1.00000e+00     1.18366e+01     1.33554e+01
       256        264     1.00000e+00     1.18350e+01     1.33533e+01
       257        265     1.00000e+00     1.18329e+01     1.81237e+01
       258        266     1.00000e+00     1.18291e+01     1.98390e+01
       259        267     1.00000e+00     1.18236e+01     2.79398e+01
       260        268     1.00000e+00     1.18208e+01     2.52599e+01
       261        269     1.00000e+00     1.18179e+01     1.39617e+01
       262        270     1.00000e+00     1.18168e+01     1.31072e+01
       263        271     1.00000e+00     1.18152e+01     1.47913e+01
       264        272     1.00000e+00     1.18123e+01     1.63075e+01
       265        273     1.00000e+00     1.18098e+01     3.07939e+01
       266        274     1.00000e+00     1.18055e+01     1.51054e+01
       267        275     1.00000e+00     1.18037e+01     1.39711e+01
       268        276     1.00000e+00     1.18016e+01     1.56333e+01
       269        277     1.00000e+00     1.17990e+01     2.24376e+01
       270        278     1.00000e+00     1.17958e+01     1.61188e+01
       271        279     1.00000e+00     1.17931e+01     1.35645e+01
       272        280     1.00000e+00     1.17914e+01     1.67677e+01
       273        281     1.00000e+00     1.17900e+01     1.64113e+01
       274        282     1.00000e+00     1.17885e+01     1.52363e+01
       275        283     1.00000e+00     1.17853e+01     1.76110e+01
       276        284     1.00000e+00     1.17832e+01     1.98259e+01
       277        285     1.00000e+00     1.17810e+01     1.75260e+01
       278        286     1.00000e+00     1.17775e+01     1.44083e+01
       279        287     1.00000e+00     1.17766e+01     1.68403e+01
       280        288     1.00000e+00     1.17756e+01     1.28973e+01
       281        289     1.00000e+00     1.17734e+01     1.33253e+01
       282        290     1.00000e+00     1.17715e+01     1.74659e+01
       283        291     1.00000e+00     1.17690e+01     1.89100e+01
       284        292     1.00000e+00     1.17675e+01     2.18503e+01
       285        293     1.00000e+00     1.17660e+01     1.39770e+01
       286        294     1.00000e+00     1.17648e+01     1.17909e+01
       287        295     1.00000e+00     1.17635e+01     1.47060e+01
       288        296     1.00000e+00     1.17614e+01     1.86143e+01
       289        297     1.00000e+00     1.17594e+01     1.88382e+01
       290        298     1.00000e+00     1.17580e+01     2.21734e+01
       291        299     1.00000e+00     1.17561e+01     1.50492e+01
       292        300     1.00000e+00     1.17543e+01     1.33365e+01
       293        301     1.00000e+00     1.17529e+01     1.51172e+01
       294        302     1.00000e+00     1.17512e+01     1.66748e+01
       295        303     1.00000e+00     1.17493e+01     1.80284e+01
       296        304     1.00000e+00     1.17475e+01     1.59713e+01
       297        305     1.00000e+00     1.17459e+01     1.22070e+01
       298        306     1.00000e+00     1.17446e+01     1.27255e+01
       299        307     1.00000e+00     1.17438e+01     1.25096e+01
       300        308     1.00000e+00     1.17425e+01     1.36868e+01
       301        309     1.00000e+00     1.17397e+01     1.50072e+01
       302        311     4.66303e-01     1.17386e+01     1.69181e+01
       303        312     1.00000e+00     1.17371e+01     1.16738e+01
       304        313     1.00000e+00     1.17358e+01     9.55563e+00
       305        314     1.00000e+00     1.17348e+01     1.21128e+01
       306        315     1.00000e+00     1.17330e+01     1.50568e+01
       307        316     1.00000e+00     1.17320e+01     2.01100e+01
       308        317     1.00000e+00     1.17300e+01     1.12229e+01
       309        318     1.00000e+00     1.17292e+01     8.17644e+00
       310        319     1.00000e+00     1.17284e+01     1.02582e+01
       311        320     1.00000e+00     1.17272e+01     1.31153e+01
       312        321     1.00000e+00     1.17254e+01     1.50403e+01
       313        322     1.00000e+00     1.17248e+01     1.96780e+01
       314        323     1.00000e+00     1.17228e+01     1.02881e+01
       315        324     1.00000e+00     1.17220e+01     8.46336e+00
       316        325     1.00000e+00     1.17212e+01     1.09020e+01
       317        326     1.00000e+00     1.17198e+01     1.39181e+01
       318        327     1.00000e+00     1.17181e+01     1.51406e+01
       319        328     1.00000e+00     1.17171e+01     1.69516e+01
       320        329     1.00000e+00     1.17159e+01     9.29539e+00
       321        330     1.00000e+00     1.17153e+01     8.57954e+00
       322        331     1.00000e+00     1.17146e+01     1.03618e+01
       323        332     1.00000e+00     1.17128e+01     1.34186e+01
       324        334     5.19029e-01     1.17117e+01     1.54914e+01
       325        335     1.00000e+00     1.17100e+01     1.07612e+01
       326        336     1.00000e+00     1.17089e+01     8.26823e+00
       327        337     1.00000e+00     1.17084e+01     1.02882e+01
       328        338     1.00000e+00     1.17078e+01     1.15064e+01
       329        339     1.00000e+00     1.17060e+01     1.27160e+01
       330        341     4.64458e-01     1.17049e+01     1.39783e+01
       331        342     1.00000e+00     1.17034e+01     8.80598e+00
       332        343     1.00000e+00     1.17025e+01     9.36689e+00
       333        344     1.00000e+00     1.17020e+01     1.07726e+01
       334        345     1.00000e+00     1.17012e+01     1.14295e+01
       335        346     1.00000e+00     1.17003e+01     1.92297e+01
       336        347     1.00000e+00     1.16984e+01     8.65726e+00
       337        348     1.00000e+00     1.16978e+01     7.45961e+00
       338        349     1.00000e+00     1.16971e+01     9.49652e+00
       339        350     1.00000e+00     1.16962e+01     1.22532e+01
       340        351     1.00000e+00     1.16947e+01     1.22609e+01
       341        352     1.00000e+00     1.16935e+01     1.72076e+01
       342        353     1.00000e+00     1.16921e+01     8.14696e+00
       343        354     1.00000e+00     1.16918e+01     6.58717e+00
       344        355     1.00000e+00     1.16913e+01     7.49983e+00
       345        356     1.00000e+00     1.16902e+01     9.51129e+00
       346        357     1.00000e+00     1.16887e+01     1.03555e+01
       347        358     1.00000e+00     1.16872e+01     1.22464e+01
       348        359     1.00000e+00     1.16864e+01     9.78641e+00
       349        360     1.00000e+00     1.16859e+01     7.76198e+00
       350        361     1.00000e+00     1.16854e+01     7.19596e+00
       351        362     1.00000e+00     1.16845e+01     8.47549e+00
       352        363     1.00000e+00     1.16836e+01     1.47472e+01
       353        364     1.00000e+00     1.16824e+01     9.42498e+00
       354        365     1.00000e+00     1.16819e+01     7.67048e+00
       355        366     1.00000e+00     1.16812e+01     9.06251e+00
       356        367     1.00000e+00     1.16802e+01     1.10583e+01
       357        368     1.00000e+00     1.16791e+01     1.07263e+01
       358        369     1.00000e+00     1.16782e+01     9.00455e+00
       359        370     1.00000e+00     1.16777e+01     8.58734e+00
       360        371     1.00000e+00     1.16773e+01     8.52974e+00
       361        372     1.00000e+00     1.16761e+01     9.10942e+00
       362        373     1.00000e+00     1.16749e+01     1.30825e+01
       363        374     1.00000e+00     1.16738e+01     1.00007e+01
       364        375     1.00000e+00     1.16732e+01     7.92157e+00
       365        376     1.00000e+00     1.16727e+01     8.40321e+00
       366        377     1.00000e+00     1.16721e+01     8.62115e+00
       367        378     1.00000e+00     1.16712e+01     9.02457e+00
       368        379     1.00000e+00     1.16707e+01     1.17025e+01
       369        380     1.00000e+00     1.16700e+01     7.10423e+00
       370        381     1.00000e+00     1.16694e+01     7.43769e+00
       371        382     1.00000e+00     1.16686e+01     9.29128e+00
       372        383     1.00000e+00     1.16675e+01     1.02631e+01
       373        385     3.25124e-01     1.16670e+01     1.04935e+01
       374        386     1.00000e+00     1.16663e+01     6.96057e+00
       375        387     1.00000e+00     1.16659e+01     6.60863e+00
       376        388     1.00000e+00     1.16654e+01     8.79623e+00
       377        389     1.00000e+00     1.16645e+01     1.09499e+01
       378        390     1.00000e+00     1.16637e+01     1.30232e+01
       379        391     1.00000e+00     1.16629e+01     8.67711e+00
       380        392     1.00000e+00     1.16625e+01     6.07895e+00
       381        393     1.00000e+00     1.16622e+01     6.80563e+00
       382        394     1.00000e+00     1.16618e+01     8.53876e+00
       383        395     1.00000e+00     1.16608e+01     1.11979e+01
       384        396     1.00000e+00     1.16600e+01     1.39845e+01
       385        397     1.00000e+00     1.16590e+01     9.06288e+00
       386        398     1.00000e+00     1.16585e+01     5.94476e+00
       387        399     1.00000e+00     1.16582e+01     6.57274e+00
       388        400     1.00000e+00     1.16578e+01     8.37614e+00
       389        401     1.00000e+00     1.16569e+01     1.09609e+01
       390        402     1.00000e+00     1.16565e+01     1.51337e+01
       391        403     1.00000e+00     1.16554e+01     8.41069e+00
       392        404     1.00000e+00     1.16551e+01     5.42269e+00
       393        405     1.00000e+00     1.16549e+01     5.69162e+00
       394        406     1.00000e+00     1.16545e+01     7.77896e+00
       395        407     1.00000e+00     1.16538e+01     1.09776e+01
       396        408     1.00000e+00     1.16528e+01     1.21141e+01
       397        409     1.00000e+00     1.16523e+01     1.40211e+01
       398        410     1.00000e+00     1.16515e+01     6.77725e+00
       399        411     1.00000e+00     1.16512e+01     4.79278e+00
       400        412     1.00000e+00     1.16510e+01     6.06121e+00
Exceeded Maximum Number of Iterations
</pre><h2 id="6">-----------------------------------------------------</h2><pre class="codeinput"><span class="comment">% Visualize weights</span>
W1 = reshape(opttheta(1:hiddenSize * inputSize), hiddenSize, inputSize);
display_network(W1');

<span class="comment">%%======================================================================</span>
</pre><img vspace="5" hspace="5" src="stlExercise_01.png" alt=""> <h2 id="7">STEP 3: Extract Features from the Supervised Dataset</h2><pre>You need to complete the code in feedForwardAutoencoder.m so that the
following command will extract features from the data.</pre><pre class="codeinput">trainFeatures = feedForwardAutoencoder(opttheta, hiddenSize, inputSize, <span class="keyword">...</span>
                                       trainData);

testFeatures = feedForwardAutoencoder(opttheta, hiddenSize, inputSize, <span class="keyword">...</span>
                                       testData);

<span class="comment">%%======================================================================</span>
</pre><h2 id="8">STEP 4: Train the softmax classifier</h2><pre class="codeinput">softmaxModel = struct;
</pre><h2 id="9">----------------- YOUR CODE HERE ----------------------</h2><pre>Use softmaxTrain.m from the previous exercise to train a multi-class
classifier.</pre><pre class="codeinput"><span class="comment">%  Use lambda = 1e-4 for the weight regularization for softmax</span>

<span class="comment">% You need to compute softmaxModel using softmaxTrain on trainFeatures and</span>
<span class="comment">% trainLabels</span>


<span class="comment">%TODO: numclasses (second parameter)?</span>
numClasses = numLabels;
softmaxModel = softmaxTrain(size(trainFeatures,1), numClasses, lambda, trainFeatures, trainLabels, options);
</pre><pre class="codeoutput"> Iteration   FunEvals     Step Length    Function Val        Opt Cond
         1          3     6.36196e-01     1.33497e+00     1.32257e+01
         2          4     1.00000e+00     4.74263e-01     4.61980e+00
         3          5     1.00000e+00     3.36611e-01     3.25911e+00
         4          6     1.00000e+00     2.56673e-01     1.51167e+00
         5          7     1.00000e+00     2.02376e-01     8.41515e-01
         6          8     1.00000e+00     1.64045e-01     8.52121e-01
         7          9     1.00000e+00     1.35332e-01     7.70914e-01
         8         10     1.00000e+00     1.21191e-01     4.26088e-01
         9         11     1.00000e+00     1.08163e-01     2.63100e-01
        10         12     1.00000e+00     9.84830e-02     2.48788e-01
        11         13     1.00000e+00     8.56455e-02     2.12026e-01
        12         14     1.00000e+00     7.10349e-02     2.32977e-01
        13         15     1.00000e+00     6.10744e-02     1.36751e-01
        14         16     1.00000e+00     5.73789e-02     1.01526e-01
        15         17     1.00000e+00     5.25205e-02     9.39663e-02
        16         18     1.00000e+00     4.78129e-02     1.20996e-01
        17         19     1.00000e+00     4.41772e-02     9.78569e-02
        18         20     1.00000e+00     3.97134e-02     8.33212e-02
        19         21     1.00000e+00     3.69123e-02     8.29278e-02
        20         22     1.00000e+00     3.50902e-02     5.13764e-02
        21         23     1.00000e+00     3.30047e-02     4.75294e-02
        22         24     1.00000e+00     3.08803e-02     5.19886e-02
        23         25     1.00000e+00     2.78050e-02     4.60122e-02
        24         27     3.93888e-01     2.65762e-02     6.44841e-02
        25         28     1.00000e+00     2.48205e-02     3.66557e-02
        26         29     1.00000e+00     2.37104e-02     2.65275e-02
        27         30     1.00000e+00     2.24124e-02     2.93779e-02
        28         31     1.00000e+00     2.12620e-02     2.98684e-02
        29         32     1.00000e+00     1.99678e-02     2.51606e-02
        30         33     1.00000e+00     1.86769e-02     2.71834e-02
        31         34     1.00000e+00     1.75297e-02     3.03202e-02
        32         35     1.00000e+00     1.67584e-02     2.43802e-02
        33         36     1.00000e+00     1.55925e-02     1.97607e-02
        34         37     1.00000e+00     1.48072e-02     2.25549e-02
        35         38     1.00000e+00     1.41815e-02     1.67298e-02
        36         39     1.00000e+00     1.32595e-02     1.67678e-02
        37         40     1.00000e+00     1.25670e-02     1.92289e-02
        38         41     1.00000e+00     1.18497e-02     2.32757e-02
        39         42     1.00000e+00     1.12050e-02     2.02210e-02
        40         43     1.00000e+00     1.07260e-02     2.02083e-02
        41         44     1.00000e+00     9.99713e-03     1.40883e-02
        42         45     1.00000e+00     9.49947e-03     1.28538e-02
        43         46     1.00000e+00     9.03008e-03     1.22851e-02
        44         47     1.00000e+00     8.40422e-03     1.09966e-02
        45         48     1.00000e+00     7.74442e-03     1.31562e-02
        46         49     1.00000e+00     7.29999e-03     8.69803e-03
        47         50     1.00000e+00     7.00117e-03     8.90130e-03
        48         51     1.00000e+00     6.62966e-03     9.49878e-03
        49         52     1.00000e+00     6.15874e-03     1.02286e-02
        50         53     1.00000e+00     5.74425e-03     1.05769e-02
        51         54     1.00000e+00     5.44024e-03     9.06070e-03
        52         55     1.00000e+00     5.26351e-03     1.06360e-02
        53         56     1.00000e+00     5.01555e-03     8.36821e-03
        54         57     1.00000e+00     4.74889e-03     5.53041e-03
        55         58     1.00000e+00     4.49073e-03     4.62436e-03
        56         59     1.00000e+00     4.26168e-03     5.21395e-03
        57         60     1.00000e+00     3.94117e-03     5.81362e-03
        58         61     1.00000e+00     3.66187e-03     7.63935e-03
        59         62     1.00000e+00     3.44681e-03     5.20709e-03
        60         63     1.00000e+00     3.29185e-03     4.80135e-03
        61         64     1.00000e+00     3.07250e-03     4.75724e-03
        62         65     1.00000e+00     2.81752e-03     4.80489e-03
        63         66     1.00000e+00     2.51722e-03     5.94375e-03
        64         70     1.25000e-01     2.46296e-03     5.46013e-03
        65         78     7.81250e-03     2.45761e-03     5.42162e-03
        66         89     9.76562e-04     2.45666e-03     5.41601e-03
        67        102     2.44141e-04     2.45637e-03     5.41449e-03
        68        119     1.52588e-05     2.45636e-03     5.41439e-03
        69        140     9.53674e-07     2.45636e-03     5.41439e-03
        70        162     4.76837e-07     2.45635e-03     5.41439e-03
Function Value changing by less than TolX
</pre><h2 id="10">-----------------------------------------------------</h2><pre class="codeinput"><span class="comment">%%======================================================================</span>
</pre><h2 id="11">STEP 5: Testing</h2><h2 id="12">----------------- YOUR CODE HERE ----------------------</h2><p>Compute Predictions on the test set (testFeatures) using softmaxPredict and softmaxModel</p><pre class="codeinput">pred = softmaxPredict(softmaxModel, testFeatures);
</pre><h2 id="13">-----------------------------------------------------</h2><pre class="codeinput"><span class="comment">% Classification Score</span>
fprintf(<span class="string">'Test Accuracy: %f%%\n'</span>, 100*mean(pred(:) == testLabels(:)));

<span class="comment">% (note that we shift the labels by 1, so that digit 0 now corresponds to</span>
<span class="comment">%  label 1)</span>
<span class="comment">%</span>
<span class="comment">% Accuracy is the proportion of correctly classified images</span>
<span class="comment">% The results for our implementation was:</span>
<span class="comment">%</span>
<span class="comment">% Accuracy: 98.3%</span>
<span class="comment">%</span>
<span class="comment">%</span>
</pre><pre class="codeoutput">Test Accuracy: 98.091254%
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2016b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% CS294A/CS294W Self-taught Learning Exercise

%  Instructions
%  REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH
% 
%  This file contains code that helps you get started on the
%  self-taught learning. You will need to complete code in feedForwardAutoencoder.m
%  You will also need to have implemented sparseAutoencoderCost.m and 
%  softmaxCost.m from previous exercises.
%
%% ======================================================================
%  STEP 0: Here we provide the relevant parameters values that will
%  allow your sparse autoencoder to get good filters; you do not need to 
%  change the parameters below.

inputSize  = 28 * 28;
numLabels  = 5;
hiddenSize = 200;
sparsityParam = 0.1; % desired average activation of the hidden units.
                     % (This was denoted by the Greek alphabet rho, which looks like a lower-case "p",
		             %  in the lecture notes). 
lambda = 3e-3;       % weight decay parameter       
beta = 3;            % weight of sparsity penalty term   
maxIter = 400;

%% ======================================================================
%  STEP 1: Load data from the MNIST database
%
%  This loads our training and test data from the MNIST database files.
%  We have sorted the data for you in this so that you will not have to
%  change it.

% Load MNIST database files
mnistData   = loadMNISTImages('train-images-idx3-ubyte');
mnistLabels = loadMNISTLabels('train-labels-idx1-ubyte');

% Set Unlabeled Set (All Images)

% Simulate a Labeled and Unlabeled set
labeledSet   = find(mnistLabels >= 0 & mnistLabels <= 4);
unlabeledSet = find(mnistLabels >= 5);

numTrain = round(numel(labeledSet)/2);
trainSet = labeledSet(1:numTrain);
testSet  = labeledSet(numTrain+1:end);

unlabeledData = mnistData(:, unlabeledSet);

trainData   = mnistData(:, trainSet);
trainLabels = mnistLabels(trainSet)' + 1; % Shift Labels to the Range 1-5

testData   = mnistData(:, testSet);
testLabels = mnistLabels(testSet)' + 1;   % Shift Labels to the Range 1-5

% Output Some Statistics
fprintf('# examples in unlabeled set: %d\n', size(unlabeledData, 2));
fprintf('# examples in supervised training set: %d\n\n', size(trainData, 2));
fprintf('# examples in supervised testing set: %d\n\n', size(testData, 2));

%% ======================================================================
%  STEP 2: Train the sparse autoencoder
%  This trains the sparse autoencoder on the unlabeled training
%  images. 

%  Randomly initialize the parameters
theta = initializeParameters(hiddenSize, inputSize);

%% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH- YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH
%  Find opttheta by running the sparse autoencoder on
%  unlabeledTrainingImages

%[cost,grad] = sparseAutoencoderCost(theta, inputSize, hiddenSize, ...
%                                lambda, sparsityParam, beta, unlabeledData);

%  Use minFunc to minimize the function
addpath minFunc/
options.Method = 'lbfgs'; % Here, we use L-BFGS to optimize our cost
                          % function. Generally, for minFunc to work, you
                          % need a function pointer with two outputs: the
                          % function value and the gradient. In our problem,
                          % sparseAutoencoderCost.m satisfies this.
options.maxIter = maxIter;	  % Maximum number of iterations of L-BFGS to run 
options.display = 'on';


[opttheta, cost] = minFunc( @(p) sparseAutoencoderCost(p, ...
                                   inputSize, hiddenSize, ...
                                   lambda, sparsityParam, ...
                                   beta, unlabeledData), ...
                              theta, options);


%% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
                          
% Visualize weights
W1 = reshape(opttheta(1:hiddenSize * inputSize), hiddenSize, inputSize);
display_network(W1');

%%======================================================================
%% STEP 3: Extract Features from the Supervised Dataset
%  
%  You need to complete the code in feedForwardAutoencoder.m so that the 
%  following command will extract features from the data.

trainFeatures = feedForwardAutoencoder(opttheta, hiddenSize, inputSize, ...
                                       trainData);

testFeatures = feedForwardAutoencoder(opttheta, hiddenSize, inputSize, ...
                                       testData);

%%======================================================================
%% STEP 4: Train the softmax classifier

softmaxModel = struct;  
%% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH- YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH
%  Use softmaxTrain.m from the previous exercise to train a multi-class
%  classifier. 

%  Use lambda = 1e-4 for the weight regularization for softmax

% You need to compute softmaxModel using softmaxTrain on trainFeatures and
% trainLabels
 

%TODO: numclasses (second parameter)?
numClasses = numLabels;
softmaxModel = softmaxTrain(size(trainFeatures,1), numClasses, lambda, trainFeatures, trainLabels, options);








%% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-


%%======================================================================
%% STEP 5: Testing 

%% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH- YOUR CODE HERE REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH
% Compute Predictions on the test set (testFeatures) using softmaxPredict
% and softmaxModel


pred = softmaxPredict(softmaxModel, testFeatures);












%% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-

% Classification Score
fprintf('Test Accuracy: %f%%\n', 100*mean(pred(:) == testLabels(:)));

% (note that we shift the labels by 1, so that digit 0 now corresponds to
%  label 1)
%
% Accuracy is the proportion of correctly classified images
% The results for our implementation was:
%
% Accuracy: 98.3%
%
% 

##### SOURCE END #####
--></body></html>